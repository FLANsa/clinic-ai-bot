"""
Performance Tests API Endpoint
"""
import logging
import time
import json
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Optional, Any
from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.orm import Session
from pydantic import BaseModel, Field
from app.db.session import get_db
from app.core.llm_client import LLMClient
from app.core.agent import ChatAgent
from app.core.models import ConversationInput

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/test/performance", tags=["Test - Performance"])

# Ù…Ø¬Ù„Ø¯ Ù„Ø­ÙØ¸ ØªÙ‚Ø§Ø±ÙŠØ± Ø§Ù„Ø£Ø¯Ø§Ø¡
PERFORMANCE_REPORTS_DIR = Path("backend/performance_reports")
PERFORMANCE_REPORTS_DIR.mkdir(parents=True, exist_ok=True)


class PerformanceResult(BaseModel):
    """Ù†ØªÙŠØ¬Ø© Ø§Ø®ØªØ¨Ø§Ø± Ø£Ø¯Ø§Ø¡ ÙˆØ§Ø­Ø¯"""
    question: str
    duration: float
    success: bool
    intent: Optional[str] = None
    error: Optional[str] = None
    db_context_used: bool = False
    exceeds_p95: bool = False
    exceeds_p99: bool = False


class PerformanceSummary(BaseModel):
    """Ù…Ù„Ø®Øµ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø£Ø¯Ø§Ø¡"""
    total_tests: int
    success_count: int
    error_count: int
    success_rate: float
    response_time: Dict[str, float]
    intents: Dict[str, int]
    results: List[PerformanceResult]
    alerts: List[Dict[str, Any]] = Field(default_factory=list)
    timestamp: str = Field(default_factory=lambda: datetime.now().isoformat())


# Test questions
TEST_QUESTIONS = [
    "Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©ØŸ",
    "Ø£Ø±ÙŠØ¯ Ø­Ø¬Ø² Ù…ÙˆØ¹Ø¯",
    "ÙƒÙ… Ø³Ø¹Ø± ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø£Ø³Ù†Ø§Ù†ØŸ",
    "Ù…Ø§ Ù‡ÙŠ Ø£ÙˆÙ‚Ø§Øª Ø§Ù„Ø¹Ù…Ù„ØŸ",
    "Ù…Ù† Ù‡Ù… Ø§Ù„Ø£Ø·Ø¨Ø§Ø¡ Ø§Ù„Ù…ØªØ§Ø­ÙˆÙ†ØŸ",
]


def get_test_agent(
    db: Session = Depends(get_db)
) -> ChatAgent:
    """Dependency Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ChatAgent Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±"""
    llm_client = LLMClient()
    return ChatAgent(
        llm_client=llm_client,
        db_session=db
    )


@router.post("/run", response_model=PerformanceSummary)
async def run_performance_tests(
    num_tests: int = 5,
    agent: ChatAgent = Depends(get_test_agent),
    db: Session = Depends(get_db)
):
    """
    ØªØ´ØºÙŠÙ„ Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡ ÙˆØ­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ Ù…Ù„ÙØ§Øª JSON/Markdown
    
    Args:
        num_tests: Ø¹Ø¯Ø¯ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ù…Ø±Ø§Ø¯ ØªØ´ØºÙŠÙ„Ù‡Ø§ (Ø§ÙØªØ±Ø§Ø¶ÙŠ: 5)
        agent: ChatAgent instance
        db: database session
    
    Returns:
        PerformanceSummary: Ù…Ù„Ø®Øµ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª
    """
    if num_tests < 1 or num_tests > 20:
        raise HTTPException(status_code=400, detail="Ø¹Ø¯Ø¯ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ø¨ÙŠÙ† 1 Ùˆ 20")
    
    questions = TEST_QUESTIONS[:num_tests]
    results: List[PerformanceResult] = []
    durations: List[float] = []
    intents_count: Dict[str, int] = {}
    success_count = 0
    error_count = 0
    
    logger.info(f"Ø¨Ø¯Ø¡ ØªØ´ØºÙŠÙ„ {num_tests} Ø§Ø®ØªØ¨Ø§Ø± Ø£Ø¯Ø§Ø¡...")
    
    for question in questions:
        try:
            conv_input = ConversationInput(
                channel="whatsapp",
                user_id="performance_test_user",
                message=question,
                locale="ar-SA"
            )
            
            start_time = time.time()
            output = await agent.handle_message(conv_input)
            duration = time.time() - start_time
            
            durations.append(duration)
            success_count += 1
            
            intent = output.intent or "unknown"
            intents_count[intent] = intents_count.get(intent, 0) + 1
            
            results.append(PerformanceResult(
                question=question,
                duration=duration,
                success=True,
                intent=intent,
                db_context_used=output.db_context_used
            ))
            
            db.commit()
            
        except Exception as e:
            error_count += 1
            error_msg = str(e)[:200]
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø³Ø¤Ø§Ù„ '{question}': {error_msg}")
            
            results.append(PerformanceResult(
                question=question,
                duration=0.0,
                success=False,
                error=error_msg
            ))
            
            db.rollback()
    
    # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
    if durations:
        sorted_durations = sorted(durations)
        n = len(sorted_durations)
        
        p50_idx = n // 2
        p95_idx = int(n * 0.95) if n > 1 else 0
        p99_idx = int(n * 0.99) if n > 1 else 0
        
        response_time = {
            "min": min(durations),
            "max": max(durations),
            "avg": sum(durations) / n,
            "p50": sorted_durations[p50_idx] if n > 0 else 0,
            "p95": sorted_durations[p95_idx] if n > 1 else sorted_durations[0],
            "p99": sorted_durations[p99_idx] if n > 1 else sorted_durations[0]
        }
        
        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø·Ù„Ø¨Ø§Øª Ø§Ù„ØªÙŠ ØªØªØ¬Ø§ÙˆØ² P95/P99
        p95_threshold = response_time["p95"]
        p99_threshold = response_time["p99"]
        alerts = []
        
        for result in results:
            if result.success and result.duration > 0:
                exceeds_p95 = result.duration > p95_threshold
                exceeds_p99 = result.duration > p99_threshold
                
                result.exceeds_p95 = exceeds_p95
                result.exceeds_p99 = exceeds_p99
                
                if exceeds_p99:
                    alerts.append({
                        "type": "p99_exceeded",
                        "severity": "high",
                        "question": result.question,
                        "duration": result.duration,
                        "threshold": p99_threshold,
                        "intent": result.intent
                    })
                elif exceeds_p95:
                    alerts.append({
                        "type": "p95_exceeded",
                        "severity": "medium",
                        "question": result.question,
                        "duration": result.duration,
                        "threshold": p95_threshold,
                        "intent": result.intent
                    })
    else:
        response_time = {
            "min": 0.0,
            "max": 0.0,
            "avg": 0.0,
            "p50": 0.0,
            "p95": 0.0,
            "p99": 0.0
        }
        alerts = []
    
    total = success_count + error_count
    success_rate = (success_count / total * 100) if total > 0 else 0.0
    
    summary = PerformanceSummary(
        total_tests=total,
        success_count=success_count,
        error_count=error_count,
        success_rate=round(success_rate, 2),
        response_time=response_time,
        intents=intents_count,
        results=results,
        alerts=alerts,
        timestamp=datetime.now().isoformat()
    )
    
    # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ Ù…Ù„ÙØ§Øª JSON Ùˆ Markdown
    try:
        _save_performance_report(summary)
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø£Ø¯Ø§Ø¡: {str(e)}")
    
    # Ø¥Ø±Ø³Ø§Ù„ ØªÙ†Ø¨ÙŠÙ‡Ø§Øª Ù„Ù„Ø·Ù„Ø¨Ø§Øª Ø§Ù„ØªÙŠ ØªØªØ¬Ø§ÙˆØ² P95/P99
    if alerts:
        _log_alerts(alerts)
    
    return summary


def _save_performance_report(summary: PerformanceSummary):
    """Ø­ÙØ¸ ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø£Ø¯Ø§Ø¡ ÙÙŠ Ù…Ù„ÙØ§Øª JSON Ùˆ Markdown"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Ø­ÙØ¸ JSON
    json_file = PERFORMANCE_REPORTS_DIR / f"performance_report_{timestamp}.json"
    with open(json_file, "w", encoding="utf-8") as f:
        json.dump(summary.model_dump(), f, ensure_ascii=False, indent=2)
    logger.info(f"ØªÙ… Ø­ÙØ¸ ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø£Ø¯Ø§Ø¡ JSON: {json_file}")
    
    # Ø­ÙØ¸ Markdown
    md_file = PERFORMANCE_REPORTS_DIR / f"performance_report_{timestamp}.md"
    with open(md_file, "w", encoding="utf-8") as f:
        f.write(_generate_markdown_report(summary))
    logger.info(f"ØªÙ… Ø­ÙØ¸ ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø£Ø¯Ø§Ø¡ Markdown: {md_file}")


def _generate_markdown_report(summary: PerformanceSummary) -> str:
    """ØªÙˆÙ„ÙŠØ¯ ØªÙ‚Ø±ÙŠØ± Markdown Ù…Ù† Ø§Ù„Ù†ØªØ§Ø¦Ø¬"""
    md = f"""# ØªÙ‚Ø±ÙŠØ± Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø¨ÙˆØª Ø§Ù„Ø°ÙƒÙŠ

**Ø§Ù„ØªØ§Ø±ÙŠØ® ÙˆØ§Ù„ÙˆÙ‚Øª:** {summary.timestamp}
**Ø¹Ø¯Ø¯ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª:** {summary.total_tests}
**Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ø¬Ø§Ø­:** {summary.success_rate}%

## ğŸ“Š Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡

### Ø£ÙˆÙ‚Ø§Øª Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© (Ø¨Ø§Ù„Ø«ÙˆØ§Ù†ÙŠ)

- **Ø§Ù„Ø£Ø¯Ù†Ù‰:** {summary.response_time['min']:.2f}s
- **Ø§Ù„Ø£Ù‚ØµÙ‰:** {summary.response_time['max']:.2f}s
- **Ø§Ù„Ù…ØªÙˆØ³Ø·:** {summary.response_time['avg']:.2f}s
- **Ø§Ù„ÙˆØ³ÙŠØ· (P50):** {summary.response_time['p50']:.2f}s
- **P95:** {summary.response_time['p95']:.2f}s
- **P99:** {summary.response_time['p99']:.2f}s

## ğŸ¯ ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù†ÙˆØ§ÙŠØ§

"""
    for intent, count in summary.intents.items():
        md += f"- **{intent}:** {count}\n"
    
    md += "\n## âš ï¸ Ø§Ù„ØªÙ†Ø¨ÙŠÙ‡Ø§Øª\n\n"
    if summary.alerts:
        for alert in summary.alerts:
            md += f"### {alert['type'].upper()} - {alert['severity'].upper()}\n"
            md += f"- **Ø§Ù„Ø³Ø¤Ø§Ù„:** {alert['question']}\n"
            md += f"- **ÙˆÙ‚Øª Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©:** {alert['duration']:.2f}s\n"
            md += f"- **Ø§Ù„Ø¹ØªØ¨Ø©:** {alert['threshold']:.2f}s\n"
            md += f"- **Ø§Ù„Ù†ÙŠØ©:** {alert.get('intent', 'N/A')}\n\n"
    else:
        md += "Ù„Ø§ ØªÙˆØ¬Ø¯ ØªÙ†Ø¨ÙŠÙ‡Ø§Øª - Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø·Ù„Ø¨Ø§Øª Ø¶Ù…Ù† Ø§Ù„Ø­Ø¯ÙˆØ¯ Ø§Ù„Ù…Ù‚Ø¨ÙˆÙ„Ø© âœ…\n\n"
    
    md += "## ğŸ“‹ ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª\n\n"
    md += "| Ø§Ù„Ø³Ø¤Ø§Ù„ | Ø§Ù„Ù†ÙŠØ© | ÙˆÙ‚Øª Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© | Ù†Ø¬Ø­ | DB Context | P95 | P99 |\n"
    md += "|---------|-------|----------------|------|------------|-----|-----|\n"
    
    for result in summary.results:
        db_status = "âœ…" if result.db_context_used else "âŒ"
        success_status = "âœ…" if result.success else "âŒ"
        p95_status = "âš ï¸" if result.exceeds_p95 else "âœ…"
        p99_status = "ğŸš¨" if result.exceeds_p99 else "âœ…"
        
        md += f"| {result.question} | {result.intent or 'N/A'} | {result.duration:.2f}s | {success_status} | {db_status} | {p95_status} | {p99_status} |\n"
    
    return md


def _log_alerts(alerts: List[Dict]):
    """ØªØ³Ø¬ÙŠÙ„ Ø§Ù„ØªÙ†Ø¨ÙŠÙ‡Ø§Øª ÙÙŠ Ø§Ù„Ù€ logs"""
    for alert in alerts:
        if alert["severity"] == "high":
            logger.warning(
                f"ğŸš¨ P99 EXCEEDED: '{alert['question']}' took {alert['duration']:.2f}s "
                f"(threshold: {alert['threshold']:.2f}s, intent: {alert.get('intent', 'N/A')})"
            )
        else:
            logger.info(
                f"âš ï¸ P95 EXCEEDED: '{alert['question']}' took {alert['duration']:.2f}s "
                f"(threshold: {alert['threshold']:.2f}s, intent: {alert.get('intent', 'N/A')})"
            )
