"""
Performance Tests - Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡
Ù‚ÙŠØ§Ø³ ÙˆÙ‚Øª Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©ØŒ Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ø¬Ø§Ø­ØŒ ÙˆØ§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…ÙˆØ§Ø±Ø¯

Ù…Ù„Ø§Ø­Ø¸Ø©: Ù‡Ø°Ù‡ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª ØªØ­ØªØ§Ø¬ GROQ_API_KEY ÙÙŠ Ø§Ù„Ø¨ÙŠØ¦Ø©
"""
import pytest
import time
import asyncio
import os
from typing import List, Dict
from fastapi.testclient import TestClient
from sqlalchemy.orm import Session
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from app.main import app
from app.core.agent import ChatAgent
from app.core.models import ConversationInput
from app.core.llm_client import LLMClient
from app.db.session import get_db
from app.db.base import Base

# Ø§Ø³ØªØ®Ø¯Ø§Ù… SQLite Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª (Ø£Ø³Ø±Ø¹ ÙˆÙ„Ø§ ÙŠØ­ØªØ§Ø¬ Ø§ØªØµØ§Ù„ Ø®Ø§Ø±Ø¬ÙŠ)
TEST_DATABASE_URL = "sqlite:///./test_performance.db"

# Ø¥Ù†Ø´Ø§Ø¡ engine Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±
test_engine = create_engine(TEST_DATABASE_URL, connect_args={"check_same_thread": False})
TestSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=test_engine)

# Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„
Base.metadata.create_all(bind=test_engine)

# Override get_db dependency
def override_get_db():
    try:
        db = TestSessionLocal()
        yield db
    finally:
        db.close()

app.dependency_overrides[get_db] = override_get_db

client = TestClient(app)


class PerformanceMetrics:
    """ØªØ®Ø²ÙŠÙ† Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø£Ø¯Ø§Ø¡"""
    def __init__(self):
        self.response_times: List[float] = []
        self.success_count = 0
        self.error_count = 0
        self.intents: Dict[str, int] = {}
    
    def add_response_time(self, duration: float):
        self.response_times.append(duration)
    
    def add_success(self):
        self.success_count += 1
    
    def add_error(self):
        self.error_count += 1
    
    def add_intent(self, intent: str):
        self.intents[intent] = self.intents.get(intent, 0) + 1
    
    def get_stats(self) -> Dict:
        if not self.response_times:
            return {"error": "Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª"}
        
        return {
            "total_requests": len(self.response_times),
            "success_count": self.success_count,
            "error_count": self.error_count,
            "success_rate": (self.success_count / (self.success_count + self.error_count) * 100) if (self.success_count + self.error_count) > 0 else 0,
            "response_time": {
                "min": min(self.response_times),
                "max": max(self.response_times),
                "avg": sum(self.response_times) / len(self.response_times),
                "p50": sorted(self.response_times)[len(self.response_times) // 2],
                "p95": sorted(self.response_times)[int(len(self.response_times) * 0.95)],
                "p99": sorted(self.response_times)[int(len(self.response_times) * 0.99)]
            },
            "intents": self.intents
        }


# Ù‚Ø§Ø¦Ù…Ø© Ø£Ø³Ø¦Ù„Ø© Ø§Ø®ØªØ¨Ø§Ø± Ù…ØªÙ†ÙˆØ¹Ø©
TEST_QUESTIONS = [
    # FAQ
    "Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©ØŸ",
    "Ù…Ø§ Ù‡ÙŠ Ø£ÙˆÙ‚Ø§Øª Ø§Ù„Ø¹Ù…Ù„ØŸ",
    "Ù‡Ù„ ØªÙ‚Ø¨Ù„ÙˆÙ† Ø§Ù„ØªØ£Ù…ÙŠÙ† Ø§Ù„Ø·Ø¨ÙŠØŸ",
    
    # Booking
    "Ø£Ø±ÙŠØ¯ Ø­Ø¬Ø² Ù…ÙˆØ¹Ø¯",
    "Ù…ØªÙ‰ Ù…ØªØ§Ø­ Ø¯. Ù…Ø­Ù…Ø¯ØŸ",
    "Ø£Ø±ÙŠØ¯ Ù…ÙˆØ¹Ø¯ Ù„ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø£Ø³Ù†Ø§Ù†",
    
    # Branch Info
    "Ø£ÙŠÙ† ÙØ±Ø¹ Ø§Ù„ÙØ§Ø®Ø±ÙŠØ©ØŸ",
    "Ù…Ø§ Ù‡ÙŠ Ø£Ø±Ù‚Ø§Ù… Ù‡ÙˆØ§ØªÙ Ø§Ù„ÙØ±ÙˆØ¹ØŸ",
    
    # Service Info
    "ÙƒÙ… Ø³Ø¹Ø± ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø£Ø³Ù†Ø§Ù†ØŸ",
    "Ù…Ø§ Ù‡ÙŠ Ù…Ø¯Ø© Ø¬Ù„Ø³Ø© Ø§Ù„Ø¹Ù„Ø§Ø¬ Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØŸ",
    
    # Doctor Info
    "Ù…Ù† Ù‡Ù… Ø§Ù„Ø£Ø·Ø¨Ø§Ø¡ Ø§Ù„Ù…ØªØ§Ø­ÙˆÙ†ØŸ",
    "Ø£Ø±ÙŠØ¯ Ø·Ø¨ÙŠØ¨ Ø£Ø³Ù†Ø§Ù†",
    
    # Price
    "ÙƒÙ… ØªÙƒÙ„ÙØ© ØªØ¨ÙŠÙŠØ¶ Ø§Ù„Ø£Ø³Ù†Ø§Ù†ØŸ",
    "Ù…Ø§ Ù‡ÙŠ Ø£Ø³Ø¹Ø§Ø± Ø§Ù„Ø®Ø¯Ù…Ø§ØªØŸ",
    
    # Greetings
    "Ù…Ø±Ø­Ø¨Ø§Ù‹",
    "Ø´ÙƒØ±Ø§Ù‹",
]


@pytest.mark.asyncio
@pytest.mark.skipif(not os.getenv("GROQ_API_KEY"), reason="ÙŠØ­ØªØ§Ø¬ GROQ_API_KEY")
async def test_single_message_performance():
    """Ø§Ø®ØªØ¨Ø§Ø± Ø£Ø¯Ø§Ø¡ Ø±Ø³Ø§Ù„Ø© ÙˆØ§Ø­Ø¯Ø©"""
    db = TestSessionLocal()
    try:
        llm_client = LLMClient()
        agent = ChatAgent(
            llm_client=llm_client,
            db_session=db,
            embeddings_client=None,  # Ø¨Ø¯ÙˆÙ† RAG Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø³Ø±ÙŠØ¹
            vector_store=None
        )
        
        conv_input = ConversationInput(
            channel="whatsapp",
            user_id="perf_test_user",
            message="Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©ØŸ",
            locale="ar-SA"
        )
        
        start_time = time.time()
        output = await agent.handle_message(conv_input)
        duration = time.time() - start_time
        
        print(f"\nâ±ï¸  ÙˆÙ‚Øª Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©: {duration:.2f} Ø«Ø§Ù†ÙŠØ©")
        print(f"âœ… Ø§Ù„Ø±Ø¯: {output.reply_text[:100]}...")
        print(f"ğŸ¯ Ø§Ù„Ù†ÙŠØ©: {output.intent}")
        
        # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† ÙˆÙ‚Øª Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ù…Ø¹Ù‚ÙˆÙ„ (< 10 Ø«ÙˆØ§Ù†ÙŠ)
        assert duration < 10, f"ÙˆÙ‚Øª Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø¨Ø·ÙŠØ¡ Ø¬Ø¯Ø§Ù‹: {duration:.2f} Ø«Ø§Ù†ÙŠØ©"
        assert output.reply_text is not None, "ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ù‡Ù†Ø§Ùƒ Ø±Ø¯"
        
    finally:
        db.close()


@pytest.mark.skipif(not os.getenv("GROQ_API_KEY"), reason="ÙŠØ­ØªØ§Ø¬ GROQ_API_KEY")
def test_api_endpoint_performance():
    """Ø§Ø®ØªØ¨Ø§Ø± Ø£Ø¯Ø§Ø¡ API endpoint"""
    metrics = PerformanceMetrics()
    
    for question in TEST_QUESTIONS[:5]:  # Ø§Ø®ØªØ¨Ø§Ø± 5 Ø£Ø³Ø¦Ù„Ø© ÙÙ‚Ø·
        start_time = time.time()
        try:
            response = client.post(
                "/test/chat",
                json={"message": question}
            )
            duration = time.time() - start_time
            
            if response.status_code == 200:
                metrics.add_success()
                data = response.json()
                metrics.add_response_time(duration)
                if data.get("intent"):
                    metrics.add_intent(data["intent"])
            else:
                metrics.add_error()
                
        except Exception as e:
            metrics.add_error()
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø³Ø¤Ø§Ù„ '{question}': {str(e)}")
    
    stats = metrics.get_stats()
    print("\n" + "="*60)
    print("ğŸ“Š Ù†ØªØ§Ø¦Ø¬ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø£Ø¯Ø§Ø¡ - API Endpoint")
    print("="*60)
    print(f"Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø·Ù„Ø¨Ø§Øª: {stats['total_requests']}")
    print(f"Ø§Ù„Ù†Ø¬Ø§Ø­: {stats['success_count']}")
    print(f"Ø§Ù„ÙØ´Ù„: {stats['error_count']}")
    print(f"Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ø¬Ø§Ø­: {stats['success_rate']:.1f}%")
    
    rt = stats['response_time']
    print(f"\nâ±ï¸  ÙˆÙ‚Øª Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©:")
    print(f"  - Ø§Ù„Ù…ØªÙˆØ³Ø·: {rt['avg']:.2f} Ø«Ø§Ù†ÙŠØ©")
    print(f"  - Ø§Ù„Ø£Ø¯Ù†Ù‰: {rt['min']:.2f} Ø«Ø§Ù†ÙŠØ©")
    print(f"  - Ø§Ù„Ø£Ù‚ØµÙ‰: {rt['max']:.2f} Ø«Ø§Ù†ÙŠØ©")
    print(f"  - P50: {rt['p50']:.2f} Ø«Ø§Ù†ÙŠØ©")
    print(f"  - P95: {rt['p95']:.2f} Ø«Ø§Ù†ÙŠØ©")
    
    print(f"\nğŸ¯ ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù†ÙˆØ§ÙŠØ§:")
    for intent, count in stats['intents'].items():
        print(f"  - {intent}: {count}")
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø£Ø¯Ø§Ø¡
    assert stats['success_rate'] >= 80, f"Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ø¬Ø§Ø­ Ù…Ù†Ø®ÙØ¶: {stats['success_rate']:.1f}%"
    assert rt['avg'] < 5, f"Ù…ØªÙˆØ³Ø· ÙˆÙ‚Øª Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø¨Ø·ÙŠØ¡: {rt['avg']:.2f} Ø«Ø§Ù†ÙŠØ©"
    assert rt['p95'] < 10, f"P95 ÙˆÙ‚Øª Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø¨Ø·ÙŠØ¡ Ø¬Ø¯Ø§Ù‹: {rt['p95']:.2f} Ø«Ø§Ù†ÙŠØ©"


@pytest.mark.asyncio
@pytest.mark.skipif(not os.getenv("GROQ_API_KEY"), reason="ÙŠØ­ØªØ§Ø¬ GROQ_API_KEY")
async def test_concurrent_requests_performance():
    """Ø§Ø®ØªØ¨Ø§Ø± Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø·Ù„Ø¨Ø§Øª Ø§Ù„Ù…ØªØ²Ø§Ù…Ù†Ø©"""
    db = TestSessionLocal()
    try:
        llm_client = LLMClient()
        agent = ChatAgent(
            llm_client=llm_client,
            db_session=db,
            embeddings_client=None,
            vector_store=None
        )
        
        # 5 Ø·Ù„Ø¨Ø§Øª Ù…ØªØ²Ø§Ù…Ù†Ø©
        questions = TEST_QUESTIONS[:5]
        
        async def process_message(question: str):
            conv_input = ConversationInput(
                channel="whatsapp",
                user_id="concurrent_test_user",
                message=question,
                locale="ar-SA"
            )
            start = time.time()
            output = await agent.handle_message(conv_input)
            duration = time.time() - start
            return duration, output
        
        start_time = time.time()
        results = await asyncio.gather(*[process_message(q) for q in questions])
        total_duration = time.time() - start_time
        
        durations = [r[0] for r in results]
        
        print(f"\nâš¡ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø·Ù„Ø¨Ø§Øª Ø§Ù„Ù…ØªØ²Ø§Ù…Ù†Ø© (5 Ø·Ù„Ø¨Ø§Øª)")
        print(f"â±ï¸  Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ: {total_duration:.2f} Ø«Ø§Ù†ÙŠØ©")
        print(f"â±ï¸  Ù…ØªÙˆØ³Ø· ÙˆÙ‚Øª ÙƒÙ„ Ø·Ù„Ø¨: {sum(durations) / len(durations):.2f} Ø«Ø§Ù†ÙŠØ©")
        print(f"ğŸš€ Ø§Ù„Ø³Ø±Ø¹Ø©: {len(questions) / total_duration:.2f} Ø·Ù„Ø¨/Ø«Ø§Ù†ÙŠØ©")
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø·Ù„Ø¨Ø§Øª Ù†Ø¬Ø­Øª
        assert all(r[1].reply_text for r in results), "ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ù‡Ù†Ø§Ùƒ Ø±Ø¯ Ù„ÙƒÙ„ Ø·Ù„Ø¨"
        assert total_duration < 30, f"Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø¨Ø·ÙŠØ¡: {total_duration:.2f} Ø«Ø§Ù†ÙŠØ©"
        
    finally:
        db.close()


@pytest.mark.skipif(not os.getenv("GROQ_API_KEY"), reason="ÙŠØ­ØªØ§Ø¬ GROQ_API_KEY")
def test_response_time_percentiles():
    """Ø§Ø®ØªØ¨Ø§Ø± ØªÙˆØ²ÙŠØ¹ Ø£ÙˆÙ‚Ø§Øª Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©"""
    metrics = PerformanceMetrics()
    
    # Ø§Ø®ØªØ¨Ø§Ø± 20 Ø³Ø¤Ø§Ù„ Ù„ØªØ­Ù„ÙŠÙ„ Ø£ÙØ¶Ù„
    for question in TEST_QUESTIONS:
        start_time = time.time()
        try:
            response = client.post(
                "/test/chat",
                json={"message": question}
            )
            duration = time.time() - start_time
            
            if response.status_code == 200:
                metrics.add_success()
                metrics.add_response_time(duration)
            else:
                metrics.add_error()
        except Exception:
            metrics.add_error()
    
    stats = metrics.get_stats()
    rt = stats['response_time']
    
    print("\n" + "="*60)
    print("ğŸ“Š ØªØ­Ù„ÙŠÙ„ ØªÙˆØ²ÙŠØ¹ Ø£ÙˆÙ‚Ø§Øª Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©")
    print("="*60)
    print(f"Ø¹Ø¯Ø¯ Ø§Ù„Ø·Ù„Ø¨Ø§Øª: {stats['total_requests']}")
    print(f"\nâ±ï¸  Ø§Ù„Ø£ÙˆÙ‚Ø§Øª (Ø¨Ø§Ù„Ø«ÙˆØ§Ù†ÙŠ):")
    print(f"  - Ø§Ù„Ù…ØªÙˆØ³Ø· (Mean): {rt['avg']:.2f}s")
    print(f"  - Ø§Ù„ÙˆØ³ÙŠØ· (Median/P50): {rt['p50']:.2f}s")
    print(f"  - P95: {rt['p95']:.2f}s")
    print(f"  - P99: {rt['p99']:.2f}s")
    print(f"  - Ø§Ù„Ø£Ø¯Ù†Ù‰: {rt['min']:.2f}s")
    print(f"  - Ø§Ù„Ø£Ù‚ØµÙ‰: {rt['max']:.2f}s")
    
    # Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ø¬ÙˆØ¯Ø©
    print(f"\nâœ… Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ø¬ÙˆØ¯Ø©:")
    print(f"  - Ø§Ù„Ù…ØªÙˆØ³Ø· < 5s: {'âœ…' if rt['avg'] < 5 else 'âŒ'} ({rt['avg']:.2f}s)")
    print(f"  - P95 < 10s: {'âœ…' if rt['p95'] < 10 else 'âŒ'} ({rt['p95']:.2f}s)")
    print(f"  - P99 < 15s: {'âœ…' if rt['p99'] < 15 else 'âŒ'} ({rt['p99']:.2f}s)")
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù…Ø¹Ø§ÙŠÙŠØ±
    assert rt['avg'] < 5, "Ø§Ù„Ù…ØªÙˆØ³Ø· ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† < 5 Ø«ÙˆØ§Ù†ÙŠ"
    assert rt['p95'] < 10, "P95 ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† < 10 Ø«ÙˆØ§Ù†ÙŠ"


@pytest.mark.skipif(not os.getenv("GROQ_API_KEY"), reason="ÙŠØ­ØªØ§Ø¬ GROQ_API_KEY")
def test_load_performance():
    """Ø§Ø®ØªØ¨Ø§Ø± Ø£Ø¯Ø§Ø¡ ØªØ­Øª Ø­Ù…Ù„ (10 Ø·Ù„Ø¨Ø§Øª Ù…ØªØªØ§Ù„ÙŠØ©)"""
    metrics = PerformanceMetrics()
    
    print("\n" + "="*60)
    print("ğŸ”¥ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø£Ø¯Ø§Ø¡ ØªØ­Øª Ø§Ù„Ø­Ù…Ù„ (10 Ø·Ù„Ø¨Ø§Øª)")
    print("="*60)
    
    for i, question in enumerate(TEST_QUESTIONS[:10], 1):
        start_time = time.time()
        try:
            response = client.post(
                "/test/chat",
                json={"message": question}
            )
            duration = time.time() - start_time
            
            if response.status_code == 200:
                metrics.add_success()
                metrics.add_response_time(duration)
                data = response.json()
                if data.get("intent"):
                    metrics.add_intent(data["intent"])
                print(f"âœ… [{i}/10] {duration:.2f}s - {data.get('intent', 'N/A')}")
            else:
                metrics.add_error()
                print(f"âŒ [{i}/10] Failed - {response.status_code}")
        except Exception as e:
            metrics.add_error()
            print(f"âŒ [{i}/10] Error: {str(e)[:50]}")
    
    stats = metrics.get_stats()
    
    print(f"\nğŸ“Š Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©:")
    print(f"  - Ø§Ù„Ù†Ø¬Ø§Ø­: {stats['success_count']}/10")
    print(f"  - Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ø¬Ø§Ø­: {stats['success_rate']:.1f}%")
    print(f"  - Ù…ØªÙˆØ³Ø· ÙˆÙ‚Øª Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©: {stats['response_time']['avg']:.2f}s")
    
    assert stats['success_rate'] >= 90, f"Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ø¬Ø§Ø­ ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† >= 90% (Ø­Ø§Ù„ÙŠØ§Ù‹: {stats['success_rate']:.1f}%)"

